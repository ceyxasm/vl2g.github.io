<!DOCTYPE html>
<!-- saved from url=(0044)http://malllabiisc.github.io/resources/kvqa/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Text to Image">
    <meta name="author" content="MALL lab">

    <title>Few-Shot VRC</title>

    <link href="./index_files/bootstrap.css" rel="stylesheet">
    <link href="./index_files/style.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="js/html5shiv.js"></script>
      <script src="js/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="container">
      <div class="header">
        <h2 class="text"><center>Few-Shot Visual Relationship Co-Localization</center></h2>
<h4 class="text"><center><a href="https://revantteotia.github.io/">Revant Teotia*</a>, <a href="https://www.linkedin.com/in/vaibhav-mishra-iitj/?originalSubdomain=in">Vaibhav Mishra*</a>, <a href="https://www.linkedin.com/in/maheshwarimayank333">Mayank Maheshwari*</a>, <a href="https://anandmishra22.github.io/">Anand Mishra</a> </center></h4>
<h6 class="text"><center>* : Equal Contribution</center></h6>
<h4 class="text"><center>Indian Institute of Technology, Jodhpur</center></h4>
<h4 class="text"><center>ICCV 2021</center></h4>

<h4 class="text"><center> [<a href="./docs/VRC-ICCV2021.pdf">Paper</a>][<a href="https://github.com/vl2g/VRC.git">Code</a>][<a href="./docs/VRC-ICCV2021-supp.pdf">Supplementry Material</a>][<a href="https://docs.google.com/presentation/d/1EvmDQXNNyBcuVhm0PmV77_INeNL_kwcF/edit?usp=sharing&ouid=102522427864788565721&rtpof=true&sd=true">Slides</a>]</center></h4>
       </div>

 <div class="container">
      <div class="header">
      <br>
<center>
<figure class="figure"> 
    <img class="figure-img" width="100%" src= "figures/fig1.png" > 

 </figure>
</center>
&nbsp;
&nbsp;
&nbsp;

</div>
      <div class="row">
        <h3>Abstract</h3>
        <p style="text-align: justify;">                   
        In this paper, given a small bag of images, each containing a common but latent predicate, we are interested in localizing visual subject-object pairs connected via the common predicate in each of the images. We refer to this novel problem as visual relationship co-localization or VRC as an abbreviation. VRC is a challenging task, even more so than the well-studied object co-localization task. This becomes further challenging when using just a few images, the model has to learn to co-localize visual subject-object pairs connected via unseen predicates. To solve VRC, we propose an optimization framework to select a common visual relationship in each image of the bag. The goal of the optimization framework is to find the optimal solution by learning visual relationship similarity across images in a few-shot setting. To obtain robust visual relationship representation, we utilize a simple yet effective technique that learns relationship embedding as a translation vector from visual subject to visual object in a shared space. Further, to learn visual relationship similarity, we utilize a proven meta-learning technique commonly used for few-shot classification tasks. Finally, to tackle the combinatorial complexity challenge arising from an exponential number of feasible solutions, we use a greedy approximation inference algorithm that selects approximately the best solution.

We extensively evaluate our proposed framework on variations of bag sizes obtained from two challenging public datasets, namely VrR-VG and VG-150, and achieve impressive visual co-localization performance.

	</p>
      </div>
      <div class="row">
        <h3>Highlights</h3>
     <ul> 
     <li> Introduced a novel task of Visual Relationship Co-Localization (VRC).</li>
     <li> Posed VRC as a labelling problem, and propose an optimization framework to solve it.</li>
     <li> Used meta-learning based approach to colocalize visual relationships on bags of images. </li>
     </ul>
               
     </div>


<!-- <div class="row">
    <h3 id="datasetE">Dataset</h3>
    <p>&nbsp;</p>
    <p>
    	<figure class="figure"> 
		    <img class="figure-img" width="100%" src= "figures/samples_data.png" > 
		      
		    <figcaption class="figure-caption">
		    	Sample images, question-ground truth answer pairs and a relevant supporting fact from our newly introduced text-KVQA dataset.
Please note that supporting fact is not explicitly provided during training and inference of our method. Rather it is mined from the largescale knowledge bases. Please refer to supplementary material for more examples.
		    </figcaption> 

 		</figure>
    </p>
    <p>
    	<figure class="figure"> 
		    <img class="figure-img" width="100%" src= "figures/dataset.png" > 
		      
		    <figcaption class="figure-caption">
		    	text-KVQA as compared to related datasets which identifies the need for reading text for VQA task. Our dataset is not only
significantly larger than these datasets, but also only dataset which identifies the need for background knowledge in answering questions.
		    </figcaption> 

 		</figure>
    </p>
</div>

<div class="row">
  <h3 id="datasetD">Dataset Downloads</h3>
  <div class="row">
    <ol type="A">
      <li>Dataset images and QA Pairs</li>
      <ol type="a">
        <li>text-KVQA <i>(scene)</i>&nbsp;[<a href="http://dosa.cds.iisc.ac.in/kvqa/text-KVQA-scene.tar.gz"><i>Images [14.6 GB]</i></a>,&nbsp;<a href="https://drive.google.com/open?id=1uJesYPfOv0IQS1GICSLOE-CzDDp1bC5S"><i>QA Pairs</i></a>]</li>
        <li>text-KVQA <i>(book)</i>&nbsp;[<a href="https://drive.google.com/open?id=1nooQXQlYfJyM8lWsDWZGDM2OPTT4P7t0"><i>Image URLs and QA Pairs</i></a>]</li>
        <li>text-KVQA <i>(movie)</i>&nbsp;[<a href="https://drive.google.com/file/d/1JqTjtARVg31tLJPM1tlgWnYfxI-JRTCw/view?usp=sharing"><i>Image URLs and QA Pairs</i></a>]</li>
      </ol>
      <li>Knowledge Bases</li>
      <ol type="a">
        <li><a href="https://drive.google.com/file/d/1uqjE2cd2vmRyFLJBQOQaLAQrheR0aMmw/view?usp=sharing">KB-business</i></a></li>
        <li><a href="https://drive.google.com/open?id=19kVWqjAYofKXkZgKcX2MmdpOZUQCuHsV">KB-book</i></a></li>
        <li><a href="https://drive.google.com/open?id=1b5e71gihr45Qj1d22Im9P1HCMhCEx_Gd">KB-movie</i></a></li>
      </ol>
    </ol>
	<a href="./README.txt">README</a>
  </div>
</div>

<hr> -->


<h3><strong><span style="font-size: 12pt;">Bibtex</span></strong></h3>
<p>Please cite this work as follows::</p>
<pre><tt>@InProceedings{teotiaMMM2021,
  author    = "Teotia, Revant and Mishra, Vaibhav and Maheshwari, Mayank and Mishra, Anand",
  title     = "Few-shot Visual Relationship Co-Localization",
  booktitle = "ICCV",
  year      = "2021",
}</tt></pre>

     </div>


<!-- <hr>



            
 <!-- <div class="row">
       <h3>People</h3>
       	<a href="https://ajeetksingh.github.io/"><u>Ajeet Kumar Singh</u></a></br>
        <a href="https://anandmishra22.github.io/"><u>Anand Mishra</u></a> <br>
        <a href="#"><u>Shashank Shekhar</u></a><br>
        <a href="#"> <u>Anirban Chakraborty</u></a> <br>
        
      </div> -->


      <!-- <div class="row">
       <h3>Acknowledgements</h3>
        <p> Authors would like to thank MHRD, Govt. of India and Intel Corporation for partly supporting this work. 
        </p>
      </div> -->
      
      

    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
  


</div></div>
</div></div></body></html>
